{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6po98qVA8bJD"
      },
      "source": [
        "# 1.Dataset Processing (Model A)\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmdZbDKjZfxw"
      },
      "source": [
        "The section immideately below contains all necessray imports."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvff21Hv8zjk",
        "outputId": "41ac4640-0dd3-4fb2-882c-35b09b6ed915"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "WARNING:root:pydrive is deprecated and no longer maintained. We recommend that you migrate your projects to pydrive2, the maintained fork of pydrive\n"
          ]
        }
      ],
      "source": [
        "#import libraries\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import json\n",
        "import copy\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "# Code to download file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kw25lI4ue3Ys"
      },
      "source": [
        "The section immideately below contains token sequence the preprosessing steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00GV37rgjxby"
      },
      "source": [
        "The loading of the Traning, Val, and Testing datasets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKvVFTpujDta",
        "outputId": "b1f48973-e0ba-4e87-e9f1-cab755a45556"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 'I would wait for a table next time, the food was that good.', 'miscellaneous', 'neutral']\n"
          ]
        }
      ],
      "source": [
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Get the file ID of the JSON file you want to open\n",
        "test_id = \"1PIaL_FR_NKabc-qKIi_RpaRwsoGgaPqw\"\n",
        "train_id = \"1o3dgEDYno-d2WYM9w3tFE_8jF8pDYzyU\"\n",
        "val_id = \"1TZZO6Pg16dI_flowwryrF0ZKfyBhrHDf\"\n",
        "\n",
        "# Download the file\n",
        "test = drive.CreateFile({'id': test_id})\n",
        "test.GetContentFile('test_data.json')\n",
        "train = drive.CreateFile({'id': train_id})\n",
        "train.GetContentFile('train_data.json')\n",
        "val = drive.CreateFile({'id': val_id})\n",
        "val.GetContentFile('val_data.json')\n",
        "\n",
        "# Read the downloaded JSON file\n",
        "with open('test_data.json', 'r') as f:\n",
        "    test_data = json.load(f)\n",
        "\n",
        "with open('train_data.json', 'r') as f:\n",
        "    train_data = json.load(f)\n",
        "\n",
        "with open('val_data.json', 'r') as f:\n",
        "    val_data = json.load(f)\n",
        "\n",
        "\n",
        "# Function to format rows from the JSON data\n",
        "def format_rows(data):\n",
        "    formatted_rows = []\n",
        "    index = 0\n",
        "    for row in data:\n",
        "        sentence = row[0]\n",
        "        aspect = row[1]\n",
        "        polarity = row[2]\n",
        "        formatted_rows.append([index, sentence, aspect, polarity])\n",
        "        index += 1\n",
        "    return formatted_rows\n",
        "\n",
        "\n",
        "rows = test_data['data']\n",
        "test_data_formatted = format_rows(rows)\n",
        "rows = train_data['data']\n",
        "train_data_formatted = format_rows(rows)\n",
        "rows = val_data['data']\n",
        "val_data_formatted = format_rows(rows)\n",
        "\n",
        "print(val_data_formatted[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tx7-djLVKJ5B"
      },
      "outputs": [],
      "source": [
        "contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\",\n",
        "                    \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\",\n",
        "                    \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\",\n",
        "                    \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\",\n",
        "                    \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\",\n",
        "                    \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
        "                    \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\",\n",
        "                    \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\n",
        "                    \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
        "                    \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\",\n",
        "                    \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\",\n",
        "                    \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\n",
        "                    \"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
        "                    \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
        "                    \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\",\n",
        "                    \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\",\n",
        "                    \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\",\n",
        "                    \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\",\n",
        "                    \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\",\n",
        "                    \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\",\n",
        "                    \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "                    \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
        "\n",
        "\n",
        "def pre_process(sent_list):\n",
        "    output = []\n",
        "    for sent in sent_list:\n",
        "        sent = sent.lower() #case-folding\n",
        "        for word, new_word in contraction_dict.items():\n",
        "            sent = sent.replace(word, new_word) #dealing with contractions\n",
        "        sent = re.sub(r'[^\\w\\s]','',sent) #removing punctuation\n",
        "        output.append(word_tokenize(sent)) #tokenization\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wvts--w7Ml1g"
      },
      "outputs": [],
      "source": [
        "# construct token index lists for input, output and target\n",
        "# i.e., to convert each sequence into corresponding index based on the index dictionary we created in the previous section\n",
        "# e.g., ['are', 'your', 'systems', 'functioning'] -> [42, 6, 2576, 2577]\n",
        "def to_index(data, to_ix):\n",
        "    input_index_list = []\n",
        "    for sent in data:\n",
        "        input_index_list.append([to_ix[w] for w in sent])\n",
        "    return input_index_list\n",
        "\n",
        "\n",
        "def make_batch(data, to_ix):\n",
        "    #cap = max(len(lst) for lst in data)\n",
        "    cap = batch_size\n",
        "    padded = []\n",
        "    for x in data:\n",
        "      if len(x) < cap:\n",
        "        w = []\n",
        "        for _ in range(cap-len(x)):\n",
        "          w.append(to_ix[\"<pad>\"])\n",
        "        padded.append(x + w)\n",
        "      else:\n",
        "        padded.append(x)\n",
        "\n",
        "    padded = torch.tensor(padded)\n",
        "\n",
        "    return padded.to(device)\n",
        "\n",
        "\n",
        "def get_batch(ipt, lbl, i, bptt):\n",
        "    seq_len = min(bptt, len(ipt) - 1 - i)\n",
        "    data = ipt[i:i+seq_len].to(device)\n",
        "    target = lbl[i:i+seq_len].to(device)\n",
        "\n",
        "    if len(data) < bptt:\n",
        "        pad_len = bptt - len(data)\n",
        "        pad_data = torch.zeros(pad_len, data.size(1), dtype=data.dtype).fill_(0).to(device)\n",
        "        data = torch.cat([data, pad_data], dim=0)\n",
        "        pad_target = torch.zeros(pad_len, target.size(1), dtype=target.dtype).fill_(0).to(device)\n",
        "        target = torch.cat([target, pad_target], dim=0)\n",
        "\n",
        "    return data, target"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if CUDA is available\n",
        "if torch.cuda.is_available():\n",
        "    print(\"CUDA is available on this device.\")\n",
        "    # Get the number of CUDA devices\n",
        "    num_gpus = torch.cuda.device_count()\n",
        "    print(f\"Number of GPUs available: {num_gpus}\")\n",
        "\n",
        "    # Loop through and print each GPU's information\n",
        "    for i in range(num_gpus):\n",
        "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
        "        print(f\"  Memory Allocated: {torch.cuda.memory_allocated(i)} bytes\")\n",
        "        print(f\"  Memory Cached: {torch.cuda.memory_reserved(i)} bytes\")\n",
        "        print(f\"  Compute Capability: {torch.cuda.get_device_properties(i).major}.{torch.cuda.get_device_properties(i).minor}\")\n",
        "else:\n",
        "    print(\"CUDA is not available. No GPU detected.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WX5LnEJiSwy8",
        "outputId": "f1d2a220-bfac-47ee-9f6c-2af643af7267"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is available on this device.\n",
            "Number of GPUs available: 1\n",
            "GPU 0: Tesla T4\n",
            "  Memory Allocated: 0 bytes\n",
            "  Memory Cached: 0 bytes\n",
            "  Compute Capability: 7.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Preprocessing(Source):\n",
        "    sentences = []\n",
        "    aspects = []\n",
        "    labels = []\n",
        "    for row in Source:\n",
        "        sentences.append(row[1])\n",
        "        aspects.append(row[2])\n",
        "        labels.append(row[3])\n",
        "\n",
        "    # Preprocessing the data using the function defined above\n",
        "    input_token_list = pre_process(sentences)\n",
        "    label_token_list = pre_process(labels)\n",
        "\n",
        "    input_token_list_n = []\n",
        "    for x in range(len(input_token_list)):\n",
        "      input_token_list_n.append(input_token_list[x] + [\"<HL>\"] + [aspects[x]])\n",
        "\n",
        "    #print(input_token_list_n)\n",
        "\n",
        "    target_token_list = label_token_list\n",
        "\n",
        "    # set up a vocab to index dictionary\n",
        "    word_to_ix = {\"<pad>\": 0, \"<HL>\": 1}\n",
        "    for sentence in input_token_list_n:\n",
        "        for word in sentence:\n",
        "            if word not in word_to_ix:\n",
        "                word_to_ix[word] = len(word_to_ix)\n",
        "    #print(word_to_ix)\n",
        "\n",
        "    label_to_index = {\n",
        "        \"positive\": 0,\n",
        "        \"negative\": 1,\n",
        "        \"neutral\": 2\n",
        "    }\n",
        "\n",
        "    input_index = to_index(input_token_list_n, word_to_ix)\n",
        "    target_index = to_index(target_token_list, label_to_index)\n",
        "\n",
        "    input = make_batch(input_index, word_to_ix)\n",
        "    #print(input.size())\n",
        "    #print(input[0])\n",
        "\n",
        "    return input, target_index, word_to_ix"
      ],
      "metadata": {
        "id": "hFXgoyT9QEPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FA2ao2l8hOg"
      },
      "source": [
        "# 2. Model Implementation (Model A)\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hq-TZVcUkgbv"
      },
      "source": [
        "Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfj-4MVmkjKC",
        "outputId": "1162c6e6-f438-4020-8acf-416d6d13afd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "80\n"
          ]
        }
      ],
      "source": [
        "OUTPUT_SIZE = 3  # target vocab size\n",
        "HIDDEN_SIZE = 512\n",
        "N_LAYERS = 1\n",
        "N_HEADS = 8\n",
        "FF_SIZE = 2048\n",
        "DROPOUT_RATE = 0.6\n",
        "CLIP = 1\n",
        "N_EPOCHS = 50\n",
        "batch_size = 80\n",
        "bptt = batch_size\n",
        "print(batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGPZU8CHWQKx"
      },
      "source": [
        "Scaled Dot Product Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIEqDDT78q39"
      },
      "outputs": [],
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, scale, dropout_rate):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "        self.scale = scale\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "\n",
        "    def forward(self, query, key, value):\n",
        "        # calculate alignment scores\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1))  # (batch_size, n_heads, query_len, value_len)\n",
        "        scores = scores / self.scale  # (batch_size, num_heads, query_len, value_len)\n",
        "\n",
        "        # calculate the attention weights (prob) from alignment scores\n",
        "        attn_probs = F.softmax(scores, dim=-1)  # (batch_size, n_heads, query_len, value_len)\n",
        "\n",
        "        # calculate context vector\n",
        "        output = torch.matmul(self.dropout(attn_probs), value)  # (batch_size, n_heads, query_len, head_dim)\n",
        "\n",
        "        return output, attn_probs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_g6IcRcnWUux"
      },
      "source": [
        "Multi-head Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3Vsi7WfWeyE"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout_rate=0.1):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % n_heads == 0, \"`d_model` should be a multiple of `n_heads`\"\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.d_k = self.d_v = d_model // n_heads  # head_dim\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model, bias=False)  # linear transformation for the Q, K and V vectors\n",
        "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "        self.attention = ScaledDotProductAttention(np.sqrt(self.d_k), dropout_rate)\n",
        "\n",
        "\n",
        "    def project(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        x = x.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)  # (batch_size, n_heads, seq_len, d_k)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "    def unproject(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.n_heads * self.d_k)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "    def forward(self, query, key, value):\n",
        "        # apply linear projections to query, key and value\n",
        "        Q = self.project(self.W_q(query))  # (batch_size, n_heads, query_len, head_dim)\n",
        "        K = self.project(self.W_k(key))  # (batch_size, n_heads, key_len, head_dim)\n",
        "        V = self.project(self.W_v(value))  # (batch_size, n_heads, value_len, head_dim)\n",
        "\n",
        "        # calculate attention weights and context vector for each of the heads\n",
        "        x, attn = self.attention(Q, K, V)\n",
        "\n",
        "        # concatenate context vector of all the heads\n",
        "        x = self.unproject(x)  # (batch_size, query_len, d_model)\n",
        "\n",
        "        # apply linear projection to concatenated context vector\n",
        "        x = self.W_o(x)\n",
        "\n",
        "        return x, attn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9ERbipxWvhq"
      },
      "source": [
        "Positional Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJzvKBqXWyqh"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, vocab_size=6000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        encoding = torch.zeros(vocab_size, d_model)\n",
        "        position = torch.arange(0, vocab_size, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, d_model, 2).float()\n",
        "            * (-math.log(10000.0) / d_model)\n",
        "        )\n",
        "        encoding[:, 0::2] = torch.sin(position * div_term)\n",
        "        encoding[:, 1::2] = torch.cos(position * div_term)\n",
        "        encoding = encoding.unsqueeze(0)\n",
        "        self.register_buffer(\"encoding\", encoding)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.encoding[:, : x.size(1), :]\n",
        "        x = self.dropout(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eJzkjM5WgZ9"
      },
      "source": [
        "Position-wise FF Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Nh0KLsqWo-j"
      },
      "outputs": [],
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout_rate=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_ff = d_ff\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        self.w_1 = nn.Linear(d_model, d_ff)\n",
        "        self.w_2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dropout(F.relu(self.w_1(x)))  # (batch_size, seq_len, d_ff)\n",
        "        x = self.w_2(x)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        # x: (batch_size, seq_len, d_model)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_930URFXR4f"
      },
      "source": [
        "Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3oTSzkAcXTmv"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout_rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_ff = d_ff\n",
        "        self.n_heads = n_heads\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        self.attn_layer = MultiHeadAttention(d_model, n_heads, dropout_rate)\n",
        "        self.attn_layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
        "\n",
        "        self.ff_layer = PositionwiseFeedForward(d_model, d_ff, dropout_rate)\n",
        "        self.ff_layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # apply self-attention\n",
        "        x1, _ = self.attn_layer(x, x, x)  # (batch_size, source_seq_len, d_model)\n",
        "\n",
        "        # apply residual connection followed by layer normalization\n",
        "        x = self.attn_layer_norm(x + self.dropout(x1))  # (batch_size, source_seq_len, d_model)\n",
        "\n",
        "        # apply position-wise feed-forward\n",
        "        x1 = self.ff_layer(x)  # (batch_size, source_seq_len, d_model)\n",
        "\n",
        "        # apply residual connection followed by layer normalization\n",
        "        x = self.ff_layer_norm(x + self.dropout(x1))  # (batch_size, source_seq_len, d_model)\n",
        "\n",
        "        # x: (batch_size, source_seq_len, d_model)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7ueE5sdbmH7"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, n_layers, n_heads, d_ff, pad_idx, dropout_rate=0.1, max_len=5000):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.d_model = d_model\n",
        "        self.n_layers = n_layers\n",
        "        self.d_ff = d_ff\n",
        "        self.pad_idx = pad_idx\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.max_len = max_len\n",
        "\n",
        "        self.tok_embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
        "        self.pos_embedding = PositionalEncoding(d_model, dropout_rate, max_len)\n",
        "        self.layers = nn.ModuleList([\n",
        "            EncoderLayer(d_model, n_heads, d_ff, dropout_rate)\n",
        "            for _ in range(n_layers)\n",
        "        ])      # designed for multiple layers\n",
        "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # apply positional encoding to token sequences\n",
        "        x = self.tok_embedding(x)  # (batch_size, source_seq_len, d_model)\n",
        "        x = self.pos_embedding(x)  # (batch_size, source_seq_len, d_model)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)  # (batch_size, source_seq_len, d_model)\n",
        "\n",
        "        x = self.layer_norm(x)  # (batch_size, source_seq_len, d_model)\n",
        "\n",
        "        # x: (batch_size, source_seq_len, d_model)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPRIMGt_XT-u"
      },
      "source": [
        "Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cn6iQQyAXVmB"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout_rate=0.1, num_classes=None):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_ff = d_ff\n",
        "        self.n_heads = n_heads\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.attn_layer = MultiHeadAttention(d_model, n_heads, dropout_rate)\n",
        "        self.attn_layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
        "        self.enc_attn_layer = MultiHeadAttention(d_model, n_heads, dropout_rate)\n",
        "        self.enc_attn_layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
        "        self.ff_layer = PositionwiseFeedForward(d_model, d_ff, dropout_rate)\n",
        "        self.ff_layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Classification layer\n",
        "        self.num_classes = num_classes\n",
        "        if self.num_classes is not None:\n",
        "            self.classifier = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, x, memory):\n",
        "        # apply self-attention\n",
        "        x1, _ = self.attn_layer(x, x, x)\n",
        "        # (batch_size, target_seq_len, d_model)\n",
        "        # apply residual connection followed by layer normalization\n",
        "        x = self.attn_layer_norm(x + self.dropout(x1))\n",
        "        # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        # apply encoder-decoder attention\n",
        "        # memory is the output from encoder block (encoder states)\n",
        "        x1, attn = self.enc_attn_layer(x, memory, memory)\n",
        "        # x1: (batch_size, target_seq_len, d_model)\n",
        "        # attn: (batch_size, n_heads, target_seq_len, source_seq_len)\n",
        "        # apply residual connection followed by layer normalization\n",
        "        x = self.enc_attn_layer_norm(x + self.dropout(x1))\n",
        "        # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        # apply position-wise feed-forward\n",
        "        x1 = self.ff_layer(x)\n",
        "        # (batch_size, target_seq_len, d_model)\n",
        "        # apply residual connection followed by layer normalization\n",
        "        x = self.ff_layer_norm(x + self.dropout(x1))\n",
        "        # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        # x: (batch_size, target_seq_len, d_model) or (batch_size, num_classes)\n",
        "        # attn: (batch_size, n_heads, target_seq_len, source_seq_len)\n",
        "        return x, attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "720P3m08bppw"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, n_layers, n_heads, d_ff, pad_idx, dropout_rate=0.1, max_len=5000):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.d_model = d_model\n",
        "        self.n_layers = n_layers\n",
        "        self.d_ff = d_ff\n",
        "        self.pad_idx = pad_idx\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.max_len = max_len\n",
        "\n",
        "        self.tok_embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
        "        self.pos_embedding = PositionalEncoding(d_model, dropout_rate, max_len)\n",
        "        self.layers = nn.ModuleList([\n",
        "            DecoderLayer(d_model, n_heads, d_ff, dropout_rate)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
        "\n",
        "\n",
        "    def forward(self, x, memory):\n",
        "        #print(\"Input tensor x:\", x)\n",
        "        #print(\"tok_embedding weight size:\", self.tok_embedding.weight.size(0))\n",
        "\n",
        "        # apply positional encoding to token sequences\n",
        "        x = self.tok_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "        x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x, attn = layer(x, memory)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        x = self.layer_norm(x)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        # x: (batch_size, target_seq_len, d_model)\n",
        "        # attn: (batch_size, n_heads, target_seq_len, source_seq_len)\n",
        "        return x, attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61gs5XVWh68h"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, encoder, decoder, num_classes, pad_idx, d_model):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.pad_idx = pad_idx\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.num_classes = num_classes\n",
        "        self.fc_out = nn.Linear(d_model, self.num_classes)\n",
        "\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        enc_output = self.encoder(src)\n",
        "        dec_output, _ = self.decoder(tgt, enc_output)\n",
        "\n",
        "        # Classification\n",
        "        classification_output = dec_output[:, -1, :]  # Take the final output representation\n",
        "        classification_output = self.fc_out(classification_output)  # Apply a linear layer\n",
        "        #classification_output = nn.functional.softmax(classification_output, dim=-1)  # Apply softmax\n",
        "\n",
        "        # classification_output: (batch_size, num_classes)\n",
        "        return classification_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhWW-8-xXV2L"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input, target_index, vocab = Preprocessing(train_data_formatted)\n",
        "val_input, val_target_index, val_vocab = Preprocessing(val_data_formatted)\n",
        "test_input, test_target_index, test_vocab = Preprocessing(test_data_formatted)"
      ],
      "metadata": {
        "id": "7Gw-YMX5bXhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydiSHBSrXX1z"
      },
      "outputs": [],
      "source": [
        "PAD_IDX = 0\n",
        "INPUT_SIZE = len(vocab)\n",
        "encoder = Encoder(INPUT_SIZE, HIDDEN_SIZE, N_LAYERS, N_HEADS, FF_SIZE, PAD_IDX, DROPOUT_RATE)\n",
        "decoder = Decoder(INPUT_SIZE, HIDDEN_SIZE, N_LAYERS, N_HEADS, FF_SIZE, PAD_IDX, DROPOUT_RATE)\n",
        "\n",
        "model = Transformer(encoder, decoder, OUTPUT_SIZE, PAD_IDX, HIDDEN_SIZE).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ku1bRYaonr5p"
      },
      "outputs": [],
      "source": [
        "def get_lr(step_num, warmup_steps=3000, d_model=512):\n",
        "    step_num = max(1, step_num)\n",
        "    arg1 = step_num ** -0.5\n",
        "    arg2 = step_num * warmup_steps ** -1.5\n",
        "    lrate = d_model ** -0.5 * min(arg1, arg2)\n",
        "    return lrate\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=get_lr(0))\n",
        "\n",
        "\n",
        "def calculate_accuracy(output, targets):\n",
        "    predictions = output.argmax(dim=1, keepdim=True)  # assuming your output includes class scores\n",
        "    correct = predictions.eq(targets.view_as(predictions)).sum().item()\n",
        "    return correct / len(targets)\n",
        "\n",
        "\n",
        "def train(early_stopping=True, patience=25):\n",
        "    model.train() # Turn on the train mode\n",
        "    step = 0\n",
        "    losses = []\n",
        "    accuracies = []\n",
        "    val_losses = []\n",
        "    val_accuracies = []\n",
        "    best_val_loss = float('inf')\n",
        "    best_model_state = None\n",
        "    epochs_without_improvement = 0\n",
        "    for epoch in range(1, N_EPOCHS + 1):\n",
        "      total_loss = 0.0\n",
        "      total_accuracy = 0.0\n",
        "      for batch, i in enumerate(range(0, input.size(0) - 1, bptt)):\n",
        "        data, targets = get_batch(input, torch.tensor(target_index), i, bptt)\n",
        "        #print(data.size(), targets.size())\n",
        "        #print(torch.isnan(data).any(), torch.isinf(data).any())\n",
        "        #print(torch.isnan(targets).any(), torch.isinf(targets).any())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data, data)\n",
        "        for name, param in model.named_parameters():\n",
        "          if not torch.isfinite(param).all():\n",
        "            print(f\"Parameter {name} contains nan or inf.\")\n",
        "        if not torch.isfinite(output).all():\n",
        "          print(\"Output contains nan or inf.\")\n",
        "        if not torch.isfinite(targets).all():\n",
        "          print(\"Targets contain nan or inf.\")\n",
        "        #print(\"OPTSIZE: \", output.size())\n",
        "        #print(output.view(-1, output.size(-1)).size(), targets.view(-1).size())\n",
        "        loss = criterion(output.view(-1, output.size(-1)), targets.view(-1))\n",
        "        #print(loss)\n",
        "        loss.backward()\n",
        "        w = torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
        "        #print(w)\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = get_lr(step + 1)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        #print(\"STEP: \", step)\n",
        "        step += 1\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_accuracy += calculate_accuracy(output, targets)\n",
        "\n",
        "      avg_loss = total_loss / len(range(0, input.size(0) - 1, bptt))\n",
        "      avg_accuracy = total_accuracy / len(range(0, input.size(0) - 1, bptt))\n",
        "      losses.append(avg_loss)\n",
        "      accuracies.append(avg_accuracy)\n",
        "\n",
        "      val_loss, val_accuracy = evaluate(model, val_input, val_target_index, criterion, bptt)\n",
        "      val_losses.append(val_loss)\n",
        "      val_accuracies.append(val_accuracy)\n",
        "\n",
        "      print(f\"Epoch {epoch}: Train Loss = {avg_loss}, Train Accuracy = {avg_accuracy}, Val Loss = {val_loss}, Val Accuracy = {val_accuracy}\")\n",
        "\n",
        "      # Early stopping\n",
        "      if early_stopping:\n",
        "          if val_loss < best_val_loss:\n",
        "              best_val_loss = val_loss\n",
        "              best_model_state = copy.deepcopy(model.state_dict())\n",
        "              epochs_without_improvement = 0\n",
        "          else:\n",
        "              epochs_without_improvement += 1\n",
        "              if epochs_without_improvement >= patience:\n",
        "                  print(f\"Early stopping after {epoch} epochs\")\n",
        "                  break\n",
        "\n",
        "    if early_stopping:\n",
        "        model.load_state_dict(best_model_state)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(losses, label='Training Loss')\n",
        "    plt.title('Training Loss over Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(accuracies, label='Training Accuracy')\n",
        "    plt.title('Training Accuracy over Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    return model, losses, accuracies, val_losses, val_accuracies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzGuzHPE87Ya"
      },
      "source": [
        "# 3.Testing and Evaluation (Model A)\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, test_input, test_target_index, criterion, bptt):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    total_loss = 0.0\n",
        "    total_accuracy = 0.0\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient tracking\n",
        "        for i in range(0, test_input.size(0) - 1, bptt):\n",
        "            data, targets = get_batch(test_input, torch.tensor(test_target_index), i, bptt)\n",
        "            output = model(data, data)\n",
        "            loss = criterion(output.view(-1, output.size(-1)), targets.view(-1))\n",
        "            total_loss += loss.item()\n",
        "            total_accuracy += calculate_accuracy(output, targets)\n",
        "\n",
        "    avg_loss = total_loss / len(range(0, test_input.size(0) - 1, bptt))\n",
        "    avg_accuracy = total_accuracy / len(range(0, test_input.size(0) - 1, bptt))\n",
        "\n",
        "    return avg_loss, avg_accuracy"
      ],
      "metadata": {
        "id": "3-Tptm4pCMtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "def perf_eval(model, test_input, test_target_index, criterion, bptt):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    total_loss = 0.0\n",
        "    total_accuracy = 0.0\n",
        "\n",
        "    all_targets = []\n",
        "    all_predictions = []\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient tracking\n",
        "        for i in range(0, test_input.size(0) - 1, bptt):\n",
        "            data, targets = get_batch(test_input, torch.tensor(test_target_index), i, bptt)\n",
        "            output = model(data, data)\n",
        "            print(output.size())\n",
        "            loss = criterion(output.view(-1, output.size(-1)), targets.view(-1))\n",
        "            total_loss += loss.item()\n",
        "            total_accuracy += calculate_accuracy(output, targets)\n",
        "\n",
        "            # Collect targets and predictions for metrics calculation\n",
        "            all_targets.extend(targets.view(-1).cpu().numpy())\n",
        "            _, predicted = torch.max(output, 1)\n",
        "            all_predictions.extend(predicted.view(-1).cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(range(0, test_input.size(0) - 1, bptt))\n",
        "    avg_accuracy = total_accuracy / len(range(0, test_input.size(0) - 1, bptt))\n",
        "\n",
        "    # Calculate precision, recall, F1 score, and MCC with zero_division set to 0\n",
        "    precision = precision_score(all_targets, all_predictions, average='weighted', zero_division=0)\n",
        "    recall = recall_score(all_targets, all_predictions, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(all_targets, all_predictions, average='weighted', zero_division=0)\n",
        "\n",
        "    return avg_loss, avg_accuracy, precision, recall, f1"
      ],
      "metadata": {
        "id": "qoCoy7DJbQr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, train_losses, train_accuracies, val_losses, val_accuracies = train()\n",
        "\n",
        "test_loss, test_accuracy, precision, recall, f1 = perf_eval(model, test_input, test_target_index, criterion, bptt)\n",
        "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}, Precision: {precision}, Recall: {recall}, F1: {f1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "6kSYVx-UMscN",
        "outputId": "f8e2ad34-6534-4800-921d-0ebd064612ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss = 1.1680684076266343, Train Accuracy = 0.3669943820224719, Val Loss = 1.0985509554545085, Val Accuracy = 0.4041666666666666\n",
            "Epoch 2: Train Loss = 1.0573077697432443, Train Accuracy = 0.4502808988764047, Val Loss = 1.1112495164076488, Val Accuracy = 0.403125\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-cc295973e571>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accuracies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperf_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_target_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbptt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}, Precision: {precision}, Recall: {recall}, F1: {f1}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-d646a5cf0a0b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(early_stopping, patience)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0mtotal_accuracy\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcalculate_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eVMOwRvmrHA"
      },
      "source": [
        "# 1. Dataset Processing (Model B)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SE1DCx0am604"
      },
      "source": [
        "The section immideately below contains all necessray imports."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZVeNYIH9IaL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e66b4019-5dd4-4a22-bbc8-3ad87ff72cec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import json\n",
        "import copy\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Code to download file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvPUZKIAnc00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e362556a-908e-44e6-c15a-0a1360849cd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 'I would wait for a table next time, the food was that good.', 'miscellaneous', 'neutral']\n"
          ]
        }
      ],
      "source": [
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Get the file ID of the JSON file you want to open\n",
        "test_id = \"1PIaL_FR_NKabc-qKIi_RpaRwsoGgaPqw\"\n",
        "train_id = \"1o3dgEDYno-d2WYM9w3tFE_8jF8pDYzyU\"\n",
        "val_id = \"1TZZO6Pg16dI_flowwryrF0ZKfyBhrHDf\"\n",
        "\n",
        "# Download the file\n",
        "test = drive.CreateFile({'id': test_id})\n",
        "test.GetContentFile('test_data.json')\n",
        "train = drive.CreateFile({'id': train_id})\n",
        "train.GetContentFile('train_data.json')\n",
        "val = drive.CreateFile({'id': val_id})\n",
        "val.GetContentFile('val_data.json')\n",
        "\n",
        "# Read the downloaded JSON file\n",
        "with open('test_data.json', 'r') as f:\n",
        "    test_data = json.load(f)\n",
        "\n",
        "with open('train_data.json', 'r') as f:\n",
        "    train_data = json.load(f)\n",
        "\n",
        "with open('val_data.json', 'r') as f:\n",
        "    val_data = json.load(f)\n",
        "\n",
        "\n",
        "def format_rows(data):\n",
        "    formatted_rows = []\n",
        "    index = 0\n",
        "    for row in data:\n",
        "        sentence = row[0]\n",
        "        aspect = row[1]\n",
        "        polarity = row[2]\n",
        "        formatted_rows.append([index, sentence, aspect, polarity])\n",
        "        index += 1\n",
        "    return formatted_rows\n",
        "\n",
        "\n",
        "rows = test_data['data']\n",
        "test_data_formatted = format_rows(rows)\n",
        "rows = train_data['data']\n",
        "train_data_formatted = format_rows(rows)\n",
        "rows = val_data['data']\n",
        "val_data_formatted = format_rows(rows)\n",
        "\n",
        "print(val_data_formatted[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpp5aIXInhXY"
      },
      "outputs": [],
      "source": [
        "contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\",\n",
        "                    \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\",\n",
        "                    \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\",\n",
        "                    \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\",\n",
        "                    \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\",\n",
        "                    \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
        "                    \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\",\n",
        "                    \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\n",
        "                    \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
        "                    \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\",\n",
        "                    \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\",\n",
        "                    \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\n",
        "                    \"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
        "                    \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
        "                    \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\",\n",
        "                    \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\",\n",
        "                    \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\",\n",
        "                    \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\",\n",
        "                    \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\",\n",
        "                    \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\",\n",
        "                    \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "                    \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
        "\n",
        "\n",
        "def generate_ngrams(ipt, n):\n",
        "    # Initialize the list to hold the n-grams\n",
        "    opt = []\n",
        "    for words in ipt:\n",
        "        ngram_list = []\n",
        "        # Generate n-grams with a step of 2 tokens before starting the next n-gram\n",
        "        for i in range(0, len(words), 3):  # Step size of 3\n",
        "            # Create the n-gram from position i to i+n\n",
        "            if i + n <= len(words):\n",
        "                ngram = words[i:i + n]\n",
        "            else:\n",
        "                # If we exceed the list length, pad the remaining elements with '<pad>'\n",
        "                ngram = words[i:] + ['<pad>'] * (n - len(words[i:]))\n",
        "\n",
        "            # Append the n-gram to the list\n",
        "            ngram_list.append(ngram)\n",
        "\n",
        "            # Break the loop when we have added the last possible n-gram that includes padding if needed\n",
        "            if i + n >= len(words):\n",
        "                break\n",
        "        opt.append(ngram_list)\n",
        "    return opt\n",
        "\n",
        "\n",
        "def pre_process(sent_list, aspect):\n",
        "    output = []\n",
        "    for sent in sent_list:\n",
        "        sent = sent.lower() #case-folding\n",
        "        for word, new_word in contraction_dict.items():\n",
        "            sent = sent.replace(word, new_word) #dealing with contractions\n",
        "        sent = re.sub(r'[^\\w\\s]','',sent) #removing punctuation\n",
        "        output.append(word_tokenize(sent)) #tokenization\n",
        "    print(output)\n",
        "\n",
        "    return generate_ngrams(output, 6)\n",
        "\n",
        "\n",
        "def pre_process_decoder(sent_list):\n",
        "    output = []\n",
        "    for sent in sent_list:\n",
        "        sent = sent.lower() #case-folding\n",
        "        for word, new_word in contraction_dict.items():\n",
        "            sent = sent.replace(word, new_word) #dealing with contractions\n",
        "        sent = re.sub(r'[^\\w\\s]','',sent) #removing punctuation\n",
        "        output.append(word_tokenize(sent)) #tokenization\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXweq886nlgJ"
      },
      "outputs": [],
      "source": [
        "def to_index_T(data, to_ix):\n",
        "    ground_truth = []\n",
        "    for sent in data:\n",
        "        ground_truth.append([to_ix[w] for w in sent])\n",
        "    return ground_truth\n",
        "\n",
        "\n",
        "def to_index_I(data, to_ix):\n",
        "    input_index_list = []\n",
        "    for sent in data:\n",
        "      buffer = []\n",
        "      for seq in sent:\n",
        "        buffer.append([to_ix[w] for w in seq])\n",
        "      input_index_list.append(buffer)\n",
        "    return input_index_list\n",
        "\n",
        "\n",
        "def make_batch(data):\n",
        "    #cap_ext = max(len(lst) for lst in data)\n",
        "    cap_ext = batch_size\n",
        "    cap_int = []\n",
        "    for x in data:\n",
        "      cap_int.append(max(len(lst) for lst in x))\n",
        "    cap_int = max(cap_int)\n",
        "    print(cap_int, cap_ext)\n",
        "\n",
        "    opt = []\n",
        "    for module in data:\n",
        "      if len(module) < cap_ext:\n",
        "        w = []\n",
        "        for _ in range(cap_ext-len(module)):\n",
        "          w.append([0, 0])\n",
        "        opt.append(module + w)\n",
        "      else:\n",
        "        opt.append(module)\n",
        "\n",
        "    #print(opt[0])\n",
        "\n",
        "    padded = []\n",
        "    for module in opt:\n",
        "      buffer = []\n",
        "      for x in module:\n",
        "        if len(x) < cap_int:\n",
        "          w = []\n",
        "          for _ in range(cap_int-len(x)):\n",
        "            w.append(0)\n",
        "          buffer.append(x + w)\n",
        "        else:\n",
        "          buffer.append(x)\n",
        "      padded.append(buffer)\n",
        "\n",
        "    #print(padded[0])\n",
        "    padded = torch.tensor(padded)\n",
        "\n",
        "    return padded.to(device)\n",
        "\n",
        "\n",
        "def make_batch_decoder(data):\n",
        "    cap = max(len(lst) for lst in data)\n",
        "    #cap = batch_size\n",
        "    padded = []\n",
        "    for x in data:\n",
        "      if len(x) < cap:\n",
        "        w = []\n",
        "        for _ in range(cap-len(x)):\n",
        "          w.append(0)\n",
        "        padded.append(x + w)\n",
        "      else:\n",
        "        padded.append(x)\n",
        "\n",
        "    padded = torch.tensor(padded)\n",
        "\n",
        "    return padded.to(device)\n",
        "\n",
        "\n",
        "def get_batch(ipt, lbl, i, bptt):\n",
        "    seq_len = min(bptt, len(ipt) - 1 - i)\n",
        "    data = ipt[i:i+seq_len].to(device)\n",
        "    target = lbl[i:i+seq_len].to(device)\n",
        "\n",
        "    if len(data) < bptt:\n",
        "        pad_len = bptt - len(data)\n",
        "        pad_data = torch.zeros(pad_len, data.size(1), data.size(2), dtype=data.dtype).fill_(0).to(device)\n",
        "        data = torch.cat([data, pad_data], dim=0)\n",
        "        pad_target = torch.zeros(pad_len, target.size(1), dtype=target.dtype).fill_(0).to(device)\n",
        "        target = torch.cat([target, pad_target], dim=0)\n",
        "\n",
        "    return data, target\n",
        "\n",
        "\n",
        "def get_batch_decoder(ipt, lbl, i, bptt):\n",
        "    seq_len = min(bptt, len(ipt) - 1 - i)\n",
        "    data = ipt[i:i+seq_len].to(device)\n",
        "    target = lbl[i:i+seq_len].to(device)\n",
        "\n",
        "    if len(data) < bptt:\n",
        "        pad_len = bptt - len(data)\n",
        "        pad_data = torch.zeros(pad_len, data.size(1), dtype=data.dtype).fill_(0).to(device)\n",
        "        data = torch.cat([data, pad_data], dim=0)\n",
        "        pad_target = torch.zeros(pad_len, target.size(1), dtype=target.dtype).fill_(0).to(device)\n",
        "        target = torch.cat([target, pad_target], dim=0)\n",
        "\n",
        "    return data, target"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Preprocessing(Source):\n",
        "    sentences = []\n",
        "    aspects = []\n",
        "    labels = []\n",
        "    for row in Source:\n",
        "      sentences.append(row[1])\n",
        "      aspects.append(row[2])\n",
        "      labels.append(row[3])\n",
        "\n",
        "    # Preprocessing the data using the function defined above\n",
        "    input_token_list_decoder = pre_process_decoder(sentences) # -> input to encoder\n",
        "\n",
        "    input_token_list_n = []   ##MOD\n",
        "    for x in range(len(input_token_list_decoder)):    ##MOD\n",
        "      input_token_list_n.append(input_token_list_decoder[x] + [\"<HL>\"] + [aspects[x]])    ##MOD\n",
        "\n",
        "    # Preprocessing the data using the function defined above\n",
        "    input_token_list = pre_process(sentences, aspects) # -> input to encoder\n",
        "    label_token_list = pre_process_decoder(labels)\n",
        "    print(input_token_list[0], label_token_list[0])\n",
        "    print(len(input_token_list), len(label_token_list))\n",
        "\n",
        "    target_token_list = label_token_list\n",
        "\n",
        "    # set up a vocab to index dictionary\n",
        "    word_to_ix = {\"<pad>\": 0, \"<HL>\": 1}    ##MOD\n",
        "    for module in input_token_list:\n",
        "      for sentence in module:\n",
        "        for word in sentence:\n",
        "            if word not in word_to_ix:\n",
        "                word_to_ix[word] = len(word_to_ix)\n",
        "\n",
        "    for sentence in input_token_list_n:\n",
        "        for word in sentence:\n",
        "            if word not in word_to_ix:\n",
        "              word_to_ix[word] = len(word_to_ix)\n",
        "\n",
        "    word_list = list(word_to_ix.keys())\n",
        "    print(word_to_ix)\n",
        "    print(len(word_to_ix))\n",
        "\n",
        "    label_to_index = {\n",
        "        \"positive\": 0,\n",
        "        \"negative\": 1,\n",
        "        \"neutral\": 2\n",
        "    }\n",
        "\n",
        "    input_index = to_index_I(input_token_list, word_to_ix)\n",
        "    print(input_index[0])\n",
        "    input_index_decoder = to_index_T(input_token_list_n, word_to_ix)    ##MOD\n",
        "    target_index = to_index_T(target_token_list, label_to_index)\n",
        "\n",
        "    input = make_batch(input_index)\n",
        "    input_decoder = make_batch_decoder(input_index_decoder)\n",
        "    print(input.size(), input_decoder.size())\n",
        "\n",
        "    return input, input_decoder, target_index, len(word_to_ix)"
      ],
      "metadata": {
        "id": "JEi0G3wQTajU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHhcUqMdm_BM"
      },
      "source": [
        "# 2. Model Implementation (Model B)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mmk86nlnFt1"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CljkrBo_nF-A"
      },
      "outputs": [],
      "source": [
        "OUTPUT_SIZE = 3  # target vocab size\n",
        "HIDDEN_SIZE = 512\n",
        "N_LAYERS = 1\n",
        "N_encoder_LAYERS = 1\n",
        "N_ca_LAYERS = 1\n",
        "N_HEADS = 8\n",
        "FF_SIZE = 2048\n",
        "DROPOUT_RATE = 0.1\n",
        "CLIP = 1\n",
        "N_EPOCHS = 50\n",
        "batch_size = 80\n",
        "bptt = batch_size\n",
        "print(batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5k_rMPLabe8U"
      },
      "outputs": [],
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, scale, dropout_rate):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "        self.scale = scale\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "\n",
        "    def forward(self, query, key, value):\n",
        "        # calculate alignment scores\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1))  # (batch_size, n_heads, query_len, value_len)\n",
        "        scores = scores / self.scale  # (batch_size, num_heads, query_len, value_len)\n",
        "\n",
        "        # calculate the attention weights (prob) from alignment scores\n",
        "        attn_probs = F.softmax(scores, dim=-1)  # (batch_size, n_heads, query_len, value_len)\n",
        "\n",
        "        # calculate context vector\n",
        "        output = torch.matmul(self.dropout(attn_probs), value)  # (batch_size, n_heads, query_len, head_dim)\n",
        "\n",
        "        # output: (batch_size, n_heads, query_len, head_dim)\n",
        "        # attn_probs: (batch_size, n_heads, query_len, value_len)\n",
        "        return output, attn_probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBM_JuQfbfVO"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout_rate=0.1):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % n_heads == 0, \"`d_model` should be a multiple of `n_heads`\"\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.d_k = self.d_v = d_model // n_heads  # head_dim\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model, bias=False)  # linear transformation for the Q, K and V vectors\n",
        "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "        self.attention = ScaledDotProductAttention(np.sqrt(self.d_k), dropout_rate)\n",
        "\n",
        "\n",
        "    def project(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        x = x.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)  # (batch_size, n_heads, seq_len, d_k)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "    def unproject(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.n_heads * self.d_k)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "    def forward(self, query, key, value):\n",
        "        # apply linear projections to query, key and value\n",
        "        Q = self.project(self.W_q(query))  # (batch_size, n_heads, query_len, head_dim)\n",
        "        K = self.project(self.W_k(key))  # (batch_size, n_heads, key_len, head_dim)\n",
        "        V = self.project(self.W_v(value))  # (batch_size, n_heads, value_len, head_dim)\n",
        "\n",
        "        # calculate attention weights and context vector for each of the heads\n",
        "        x, attn = self.attention(Q, K, V)\n",
        "\n",
        "        # concatenate context vector of all the heads\n",
        "        x = self.unproject(x)  # (batch_size, query_len, d_model)\n",
        "\n",
        "        # apply linear projection to concatenated context vector\n",
        "        x = self.W_o(x)\n",
        "\n",
        "        return x, attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iei1Il1kbg7R"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, vocab_size=6000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        encoding = torch.zeros(vocab_size, d_model)\n",
        "        position = torch.arange(0, vocab_size, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, d_model, 2).float()\n",
        "            * (-math.log(10000.0) / d_model)\n",
        "        )\n",
        "        encoding[:, 0::2] = torch.sin(position * div_term)\n",
        "        encoding[:, 1::2] = torch.cos(position * div_term)\n",
        "        encoding = encoding.unsqueeze(0)\n",
        "        self.register_buffer(\"encoding\", encoding)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.encoding[:, : x.size(1), :]\n",
        "        x = self.dropout(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgzQIvmybhm9"
      },
      "outputs": [],
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout_rate=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_ff = d_ff\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        self.w_1 = nn.Linear(d_model, d_ff)\n",
        "        self.w_2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dropout(F.relu(self.w_1(x)))  # (batch_size, seq_len, d_ff)\n",
        "        x = self.w_2(x)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        # x: (batch_size, seq_len, d_model)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n46OBIIzbh1u"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout_rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_ff = d_ff\n",
        "        self.n_heads = n_heads\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        self.attn_layer = MultiHeadAttention(d_model, n_heads, dropout_rate)\n",
        "        self.attn_layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
        "\n",
        "        self.ff_layer = PositionwiseFeedForward(d_model, d_ff, dropout_rate)\n",
        "        self.ff_layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # apply self-attention\n",
        "        x1, _ = self.attn_layer(x, x, x)  # (batch_size, source_seq_len, d_model)\n",
        "\n",
        "        # apply residual connection followed by layer normalization\n",
        "        x = self.attn_layer_norm(x + self.dropout(x1))  # (batch_size, source_seq_len, d_model)\n",
        "\n",
        "        #x1 = self.ff_layer(x)  # (batch_size, source_seq_len, d_model)\n",
        "        #x = self.ff_layer_norm(x + self.dropout(x1))  # (batch_size, source_seq_len, d_model)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sV6Pz4zNotA9"
      },
      "outputs": [],
      "source": [
        "class CrossAttentionEncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout_rate=0.1):\n",
        "        super(CrossAttentionEncoderLayer, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_ff = d_ff\n",
        "        self.n_heads = n_heads\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        self.attn_layer = MultiHeadAttention(d_model, n_heads, dropout_rate)\n",
        "        self.attn_layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
        "\n",
        "        self.ff_layer = PositionwiseFeedForward(d_model, d_ff, dropout_rate)\n",
        "        self.ff_layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # apply self-attention\n",
        "        x1, _ = self.attn_layer(x, x, x)  # (batch_size, source_seq_len, d_model)\n",
        "\n",
        "        # apply residual connection followed by layer normalization\n",
        "        x = self.attn_layer_norm(x + self.dropout(x1))  # (batch_size, source_seq_len, d_model)\n",
        "\n",
        "        # apply position-wise feed-forward\n",
        "        x1 = self.ff_layer(x)  # (batch_size, source_seq_len, d_model)\n",
        "\n",
        "        # apply residual connection followed by layer normalization\n",
        "        x = self.ff_layer_norm(x + self.dropout(x1))  # (batch_size, source_seq_len, d_model)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQ-uQhB1pQ3h"
      },
      "outputs": [],
      "source": [
        "class CrossAttentionEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, n_encoder_layers, n_heads, d_ff, pad_idx, dropout_rate=0.1, max_len=5000):\n",
        "        super(CrossAttentionEncoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.d_model = d_model\n",
        "        self.n_encoder_layers = n_encoder_layers\n",
        "        self.d_ff = d_ff\n",
        "        self.pad_idx = pad_idx\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.max_len = max_len\n",
        "        self.layers = nn.ModuleList([\n",
        "            CrossAttentionEncoderLayer(d_model, n_heads, d_ff, dropout_rate)\n",
        "            for _ in range(n_encoder_layers)\n",
        "        ])\n",
        "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer_norm(x)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)  # (batch_size, source_seq_len, d_model)\n",
        "\n",
        "        x = self.layer_norm(x)  # (batch_size, source_seq_len, d_model)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rpc3fHubxUq"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, n_encoder_layers, n_heads, d_ff, pad_idx, dropout_rate=0.1, max_len=5000):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.d_model = d_model\n",
        "        self.n_encoder_layers = n_encoder_layers\n",
        "        self.d_ff = d_ff\n",
        "        self.pad_idx = pad_idx\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.max_len = max_len\n",
        "\n",
        "        self.tok_embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
        "        self.pos_embedding = PositionalEncoding(d_model, dropout_rate, max_len)\n",
        "        self.layers = nn.ModuleList([\n",
        "            EncoderLayer(d_model, n_heads, d_ff, dropout_rate)\n",
        "            for _ in range(n_encoder_layers)\n",
        "        ])\n",
        "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # apply positional encoding to token sequences\n",
        "        x = self.tok_embedding(x)  # (batch_size, source_seq_len, d_model)\n",
        "        x = self.pos_embedding(x)  # (batch_size, source_seq_len, d_model)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)  # (batch_size, source_seq_len, d_model)\n",
        "\n",
        "        x = self.layer_norm(x)  # (batch_size, source_seq_len, d_model)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCrMFu2ebiKs"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout_rate=0.1, num_classes=None):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_ff = d_ff\n",
        "        self.n_heads = n_heads\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.attn_layer = MultiHeadAttention(d_model, n_heads, dropout_rate)\n",
        "        self.attn_layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
        "        self.enc_attn_layer = MultiHeadAttention(d_model, n_heads, dropout_rate)\n",
        "        self.enc_attn_layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
        "        self.ff_layer = PositionwiseFeedForward(d_model, d_ff, dropout_rate)\n",
        "        self.ff_layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Classification layer\n",
        "        self.num_classes = num_classes\n",
        "        if self.num_classes is not None:\n",
        "            self.classifier = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, x, memory):\n",
        "        # apply self-attention\n",
        "        x1, _ = self.attn_layer(x, x, x)\n",
        "        # (batch_size, target_seq_len, d_model)\n",
        "        # apply residual connection followed by layer normalization\n",
        "        x = self.attn_layer_norm(x + self.dropout(x1))\n",
        "        # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        # memory is the output from encoder block (encoder states)\n",
        "        x1, attn = self.enc_attn_layer(x, memory, memory)\n",
        "        # x1: (batch_size, target_seq_len, d_model)\n",
        "        # attn: (batch_size, n_heads, target_seq_len, source_seq_len)\n",
        "        # apply residual connection followed by layer normalization\n",
        "        x = self.enc_attn_layer_norm(x + self.dropout(x1))\n",
        "        # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        # apply position-wise feed-forward\n",
        "        x1 = self.ff_layer(x)\n",
        "        # (batch_size, target_seq_len, d_model)\n",
        "        # apply residual connection followed by layer normalization\n",
        "        x = self.ff_layer_norm(x + self.dropout(x1))\n",
        "        # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        return x, attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kswFcOBHbiWG"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, n_layers, n_heads, d_ff, pad_idx, dropout_rate=0.1, max_len=5000):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.d_model = d_model\n",
        "        self.n_layers = n_layers\n",
        "        self.d_ff = d_ff\n",
        "        self.pad_idx = pad_idx\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.max_len = max_len\n",
        "\n",
        "        self.tok_embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
        "        self.pos_embedding = PositionalEncoding(d_model, dropout_rate, max_len)\n",
        "        self.layers = nn.ModuleList([\n",
        "            DecoderLayer(d_model, n_heads, d_ff, dropout_rate)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
        "\n",
        "\n",
        "    def forward(self, x, memory):\n",
        "        #print(\"Input tensor x:\", x)\n",
        "        #print(\"tok_embedding weight size:\", self.tok_embedding.weight.size(0))\n",
        "\n",
        "        # apply positional encoding to token sequences\n",
        "        x = self.tok_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "        x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x, attn = layer(x, memory)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        x = self.layer_norm(x)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        return x, attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "raOWbmJEbina"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, encoder, decoder, ca_encoder, num_classes, pad_idx, d_model):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.pad_idx = pad_idx\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.ca_encoder = ca_encoder\n",
        "        self.num_classes = num_classes\n",
        "        self.fc_out = nn.Linear(d_model, self.num_classes)\n",
        "\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        enc_output = self.encoder(src[0])\n",
        "\n",
        "        for module in src[1:]:\n",
        "          enc_output += self.encoder(module)\n",
        "\n",
        "        enc_output = self.ca_encoder(enc_output)\n",
        "        #print(\"CAEN SHAPE: \", enc_output.size())\n",
        "        dec_output, _ = self.decoder(tgt, enc_output)\n",
        "\n",
        "        # Classification\n",
        "        classification_output = dec_output[:, -1, :]  # Take the final output representation\n",
        "        classification_output = self.fc_out(classification_output)  # Apply a linear layer\n",
        "        #classification_output = nn.functional.softmax(classification_output, dim=-1)  # Apply softmax\n",
        "\n",
        "        return classification_output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input, input_decoder, target_index, vocab_size = Preprocessing(train_data_formatted)\n",
        "val_input, val_input_decoder, val_target_index, val_vocab_size = Preprocessing(val_data_formatted)\n",
        "test_input, test_input_decoder, test_target_index, test_vocab_size = Preprocessing(test_data_formatted)"
      ],
      "metadata": {
        "id": "4tAuKRYtms16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7vav-tpbjRz"
      },
      "outputs": [],
      "source": [
        "PAD_IDX = 0\n",
        "INPUT_SIZE = vocab_size\n",
        "encoder = Encoder(INPUT_SIZE, HIDDEN_SIZE, N_encoder_LAYERS, N_HEADS, FF_SIZE, PAD_IDX, DROPOUT_RATE)\n",
        "decoder = Decoder(INPUT_SIZE, HIDDEN_SIZE, N_LAYERS, N_HEADS, FF_SIZE, PAD_IDX, DROPOUT_RATE)\n",
        "ca_encoder = CrossAttentionEncoder(INPUT_SIZE, HIDDEN_SIZE, N_ca_LAYERS, N_HEADS, FF_SIZE, PAD_IDX, DROPOUT_RATE)\n",
        "\n",
        "model = Transformer(encoder, decoder, ca_encoder, OUTPUT_SIZE, PAD_IDX, HIDDEN_SIZE).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYgH1LyccfkN"
      },
      "outputs": [],
      "source": [
        "def get_lr(step_num, warmup_steps=3000, d_model=512):\n",
        "    step_num = max(1, step_num)\n",
        "    arg1 = step_num ** -0.5\n",
        "    arg2 = step_num * warmup_steps ** -1.5\n",
        "    lrate = d_model ** -0.5 * min(arg1, arg2)\n",
        "    return lrate\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=get_lr(0))\n",
        "\n",
        "losses = []\n",
        "accuracies = []\n",
        "\n",
        "\n",
        "def calculate_accuracy(output, targets):\n",
        "    predictions = output.argmax(dim=1, keepdim=True)  # assuming your output includes class scores\n",
        "    correct = predictions.eq(targets.view_as(predictions)).sum().item()\n",
        "    return correct / len(targets)\n",
        "\n",
        "\n",
        "def train(early_stopping=True, patience=25):\n",
        "    model.train() # Turn on the train mode\n",
        "    step = 0\n",
        "    losses = []\n",
        "    accuracies = []\n",
        "    val_losses = []\n",
        "    val_accuracies = []\n",
        "    best_val_loss = float('inf')\n",
        "    best_model_state = None\n",
        "    epochs_without_improvement = 0\n",
        "    for epoch in range(1, N_EPOCHS + 1):\n",
        "      total_loss = 0.0\n",
        "      total_accuracy = 0.0\n",
        "      #print(\"INPUT SIZE: \", input.size(0))\n",
        "      for batch, i in enumerate(range(0, input.size(0) - 1, bptt)):\n",
        "        #print(\"I: \", i)\n",
        "        data, targets = get_batch(input, torch.tensor(target_index), i, bptt)\n",
        "        data_decoder, _ = get_batch_decoder(input_decoder, torch.tensor(target_index), i, bptt)\n",
        "        #print(data.size(), targets.size(), data_decoder.size())\n",
        "        #print(data, targets)\n",
        "        #print(torch.isnan(data).any(), torch.isinf(data).any())\n",
        "        #print(torch.isnan(targets).any(), torch.isinf(targets).any())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data, data_decoder)\n",
        "        for name, param in model.named_parameters():\n",
        "          if not torch.isfinite(param).all():\n",
        "            print(f\"Parameter {name} contains nan or inf.\")\n",
        "        if not torch.isfinite(output).all():\n",
        "          print(\"Output contains nan or inf.\")\n",
        "        if not torch.isfinite(targets).all():\n",
        "          print(\"Targets contain nan or inf.\")\n",
        "        #print(\"OPTSIZE: \", output.size())\n",
        "        #print(output.view(-1, output.size(-1)).size(), targets.view(-1).size())\n",
        "        loss = criterion(output.view(-1, output.size(-1)), targets.view(-1))\n",
        "        #print(loss)\n",
        "        loss.backward()\n",
        "        w = torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
        "        #print(w)\n",
        "\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = get_lr(step + 1)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        #print(\"STEP: \", step)\n",
        "        step += 1\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_accuracy += calculate_accuracy(output, targets)\n",
        "\n",
        "      avg_loss = total_loss / len(range(0, input.size(0) - 1, bptt))\n",
        "      avg_accuracy = total_accuracy / len(range(0, input.size(0) - 1, bptt))\n",
        "      losses.append(avg_loss)\n",
        "      accuracies.append(avg_accuracy)\n",
        "\n",
        "      val_loss, val_accuracy = evaluate(model, val_input, val_input_decoder, val_target_index, criterion, bptt)\n",
        "      val_losses.append(val_loss)\n",
        "      val_accuracies.append(val_accuracy)\n",
        "\n",
        "      print(f\"Epoch {epoch}: Train Loss = {avg_loss}, Train Accuracy = {avg_accuracy}, Val Loss = {val_loss}, Val Accuracy = {val_accuracy}\")\n",
        "\n",
        "      # Early stopping\n",
        "      if early_stopping:\n",
        "          if val_loss < best_val_loss:\n",
        "              best_val_loss = val_loss\n",
        "              best_model_state = copy.deepcopy(model.state_dict())\n",
        "              epochs_without_improvement = 0\n",
        "          else:\n",
        "              epochs_without_improvement += 1\n",
        "              if epochs_without_improvement >= patience:\n",
        "                  print(f\"Early stopping after {epoch} epochs\")\n",
        "                  break\n",
        "\n",
        "    if early_stopping:\n",
        "        model.load_state_dict(best_model_state)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(losses, label='Training Loss')\n",
        "    plt.title('Training Loss over Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(accuracies, label='Training Accuracy')\n",
        "    plt.title('Training Accuracy over Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    return model, losses, accuracies, val_losses, val_accuracies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtRrh-0gnHFw"
      },
      "source": [
        "# Testing and Evaluation (Model B)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, test_input, test_input_decoder, test_target_index, criterion, bptt):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    total_loss = 0.0\n",
        "    total_accuracy = 0.0\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient tracking\n",
        "        for i in range(0, test_input.size(0) - 1, bptt):\n",
        "            data, targets = get_batch(test_input, torch.tensor(test_target_index), i, bptt)\n",
        "            data_decoder, _ = get_batch_decoder(test_input_decoder, torch.tensor(test_target_index), i, bptt)\n",
        "            output = model(data, data_decoder)\n",
        "            loss = criterion(output.view(-1, output.size(-1)), targets.view(-1))\n",
        "            total_loss += loss.item()\n",
        "            total_accuracy += calculate_accuracy(output, targets)\n",
        "\n",
        "    avg_loss = total_loss / len(range(0, test_input.size(0) - 1, bptt))\n",
        "    avg_accuracy = total_accuracy / len(range(0, test_input.size(0) - 1, bptt))\n",
        "\n",
        "    return avg_loss, avg_accuracy"
      ],
      "metadata": {
        "id": "bvRItW9Ac1WN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "def perf_eval(model, test_input, test_input_decoder, test_target_index, criterion, bptt):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    total_loss = 0.0\n",
        "    total_accuracy = 0.0\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient tracking\n",
        "        for i in range(0, test_input.size(0) - 1, bptt):\n",
        "            data, targets = get_batch(test_input, torch.tensor(test_target_index), i, bptt)\n",
        "            data_decoder, _ = get_batch_decoder(test_input_decoder, torch.tensor(test_target_index), i, bptt)\n",
        "            output = model(data, data_decoder)\n",
        "            loss = criterion(output.view(-1, output.size(-1)), targets.view(-1))\n",
        "            total_loss += loss.item()\n",
        "            total_accuracy += calculate_accuracy(output, targets)\n",
        "\n",
        "            predictions = output.argmax(dim=-1).cpu().numpy().flatten()\n",
        "            all_predictions.extend(predictions)\n",
        "            all_targets.extend(targets.cpu().numpy().flatten())\n",
        "\n",
        "    avg_loss = total_loss / len(range(0, test_input.size(0) - 1, bptt))\n",
        "    avg_accuracy = total_accuracy / len(range(0, test_input.size(0) - 1, bptt))\n",
        "    precision = precision_score(all_targets, all_predictions, average='weighted', zero_division=0)\n",
        "    recall = recall_score(all_targets, all_predictions, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(all_targets, all_predictions, average='weighted', zero_division=0)\n",
        "\n",
        "    return avg_loss, avg_accuracy, precision, recall, f1"
      ],
      "metadata": {
        "id": "JzUDFLuYp8hw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, train_losses, train_accuracies, val_losses, val_accuracies = train()\n",
        "\n",
        "test_loss, test_accuracy, precision, recall, f1 = perf_eval(model, test_input, test_input_decoder, test_target_index, criterion, bptt)\n",
        "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}, Precision: {precision}, Recall: {recall}, F1: {f1}\")"
      ],
      "metadata": {
        "id": "w-8p7A4lc1yT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Processing (Model C)"
      ],
      "metadata": {
        "id": "twFbBRbOz8M7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import json\n",
        "import copy\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Set the device for PyTorch (GPU if available, else CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Code to download file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "metadata": {
        "id": "ymElyLS40Vkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Get the file ID of the JSON file you want to open\n",
        "test_id = \"1PIaL_FR_NKabc-qKIi_RpaRwsoGgaPqw\"\n",
        "train_id = \"1o3dgEDYno-d2WYM9w3tFE_8jF8pDYzyU\"\n",
        "val_id = \"1TZZO6Pg16dI_flowwryrF0ZKfyBhrHDf\"\n",
        "\n",
        "# Download the file\n",
        "test = drive.CreateFile({'id': test_id})\n",
        "test.GetContentFile('test_data.json')\n",
        "train = drive.CreateFile({'id': train_id})\n",
        "train.GetContentFile('train_data.json')\n",
        "val = drive.CreateFile({'id': val_id})\n",
        "val.GetContentFile('val_data.json')\n",
        "\n",
        "# Read the downloaded JSON file\n",
        "with open('test_data.json', 'r') as f:\n",
        "    test_data = json.load(f)\n",
        "\n",
        "with open('train_data.json', 'r') as f:\n",
        "    train_data = json.load(f)\n",
        "\n",
        "with open('val_data.json', 'r') as f:\n",
        "    val_data = json.load(f)\n",
        "\n",
        "\n",
        "def format_rows(data):\n",
        "    formatted_rows = []\n",
        "    index = 0\n",
        "    for row in data:\n",
        "        sentence = row[0]\n",
        "        aspect = row[1]\n",
        "        polarity = row[2]\n",
        "        formatted_rows.append([index, sentence, aspect, polarity])\n",
        "        index += 1\n",
        "    return formatted_rows\n",
        "\n",
        "\n",
        "rows = test_data['data']\n",
        "test_data_formatted = format_rows(rows)\n",
        "rows = train_data['data']\n",
        "train_data_formatted = format_rows(rows)\n",
        "rows = val_data['data']\n",
        "val_data_formatted = format_rows(rows)\n",
        "\n",
        "print(val_data_formatted[0])\n",
        "\n",
        "def unique_pairs(data_list):\n",
        "    # A set to hold unique elements\n",
        "    unique_elements = set()\n",
        "\n",
        "    # Iterate over each sublist in the main list\n",
        "    for entry in data_list:\n",
        "        # Check if the sublist is long enough to have a second element\n",
        "        if len(entry) > 1:\n",
        "            # Add the second element to the set\n",
        "            unique_elements.add(entry[1])\n",
        "\n",
        "    opt = {}\n",
        "    for unit in unique_elements:\n",
        "        matching_entries = set()\n",
        "\n",
        "        for entry in data_list:\n",
        "            # Check if the sublist is long enough and if the second element matches the target sentence\n",
        "            if len(entry) > 1 and entry[1] == unit:\n",
        "                matching_entries.add(entry[2])\n",
        "\n",
        "        opt[unit] = matching_entries\n",
        "\n",
        "    return opt"
      ],
      "metadata": {
        "id": "QpW6k00CVLqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\",\n",
        "                    \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\",\n",
        "                    \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\",\n",
        "                    \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\",\n",
        "                    \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\",\n",
        "                    \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
        "                    \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\",\n",
        "                    \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\n",
        "                    \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
        "                    \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\",\n",
        "                    \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\",\n",
        "                    \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\n",
        "                    \"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
        "                    \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
        "                    \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\",\n",
        "                    \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\",\n",
        "                    \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\",\n",
        "                    \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\",\n",
        "                    \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\",\n",
        "                    \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\",\n",
        "                    \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "                    \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
        "\n",
        "\n",
        "def set_to_string(elements_set):\n",
        "    # Convert each element in the set to a string (in case there are non-string types)\n",
        "    elements_set = map(str, elements_set)\n",
        "\n",
        "    # Join elements with a space as the separator\n",
        "    result_string = ' '.join(elements_set)\n",
        "\n",
        "    return result_string\n",
        "\n",
        "\n",
        "def pre_process(sent_list, ap):\n",
        "    output = []\n",
        "    for unit in sent_list:\n",
        "        sent = unit.lower() #case-folding\n",
        "        for word, new_word in contraction_dict.items():\n",
        "            sent = sent.replace(word, new_word) #dealing with contractions\n",
        "        sent = re.sub(r'[^\\w\\s]','',sent) #removing punctuation\n",
        "        front = word_tokenize(sent)\n",
        "        back = word_tokenize(set_to_string(ap[unit]).lower())\n",
        "        opt = front + [\"<HL>\"] + back\n",
        "        output.append(opt)\n",
        "    return output\n",
        "\n",
        "\n",
        "def pre_process_l(sent_list):\n",
        "    output = []\n",
        "    for sent in sent_list:\n",
        "        sent = sent.lower() #case-folding\n",
        "        for word, new_word in contraction_dict.items():\n",
        "            sent = sent.replace(word, new_word) #dealing with contractions\n",
        "        sent = re.sub(r'[^\\w\\s]','',sent) #removing punctuation\n",
        "        output.append(word_tokenize(sent)) #tokenization\n",
        "    return output"
      ],
      "metadata": {
        "id": "WJPAKkbRVMcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_index(data, to_ix):\n",
        "    input_index_list = []\n",
        "    for sent in data:\n",
        "        input_index_list.append([to_ix[w] for w in sent])\n",
        "    return input_index_list\n",
        "\n",
        "\n",
        "def make_batch(data):\n",
        "    #cap = max(len(lst) for lst in data)\n",
        "    cap = 80    # This should be equal to batch_size\n",
        "    padded = []\n",
        "    for x in data:\n",
        "      if len(x) < cap:\n",
        "        w = []\n",
        "        for _ in range(cap-len(x)):\n",
        "          w.append(0)\n",
        "        padded.append(x + w)\n",
        "      else:\n",
        "        padded.append(x)\n",
        "\n",
        "    padded = torch.tensor(padded)\n",
        "\n",
        "    return padded.to(device)\n",
        "\n",
        "\n",
        "def get_batch(ipt, lbl, i, bptt):\n",
        "    seq_len = min(bptt, len(ipt) - 1 - i)\n",
        "    data = ipt[i:i+seq_len].to(device)\n",
        "    target = lbl[i:i+seq_len].to(device)\n",
        "\n",
        "    if len(data) < bptt:\n",
        "        pad_len = bptt - len(data)\n",
        "        pad_data = torch.zeros(pad_len, data.size(1), dtype=data.dtype).fill_(0).to(device)\n",
        "        data = torch.cat([data, pad_data], dim=0)\n",
        "        pad_target = torch.zeros(pad_len, target.size(1), dtype=target.dtype).fill_(0).to(device)\n",
        "        target = torch.cat([target, pad_target], dim=0)\n",
        "\n",
        "    return data, target"
      ],
      "metadata": {
        "id": "UQClqSMNVRVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Preprocessing(Source):\n",
        "    sentences = []\n",
        "    aspects = []\n",
        "    labels = []\n",
        "    for row in Source:\n",
        "      sentences.append(row[1])\n",
        "      aspects.append(row[2])\n",
        "      labels.append(row[3])\n",
        "\n",
        "    ap = unique_pairs(Source)\n",
        "\n",
        "    # Preprocessing the data using the function defined above\n",
        "    input_token_list = pre_process(sentences, ap) # -> input to encoder\n",
        "    label_token_list = pre_process_l(labels)\n",
        "\n",
        "    #print(input_token_list)\n",
        "\n",
        "    output_token_list = label_token_list\n",
        "    target_token_list = label_token_list\n",
        "\n",
        "    # set up a vocab to index dictionary\n",
        "    word_to_ix = {\"<pad>\": 0, \"<HL>\": 1}\n",
        "    for sentence in input_token_list+output_token_list:\n",
        "        for word in sentence:\n",
        "            if word not in word_to_ix:\n",
        "                word_to_ix[word] = len(word_to_ix)\n",
        "    word_list = list(word_to_ix.keys())\n",
        "    #print(word_to_ix)\n",
        "    #print(len(word_to_ix))\n",
        "\n",
        "    label_to_index = {\n",
        "        \"positive\": 0,\n",
        "        \"negative\": 1,\n",
        "        \"neutral\": 2\n",
        "    }\n",
        "\n",
        "    input_index = to_index(input_token_list, word_to_ix)\n",
        "    target_index = to_index(target_token_list, label_to_index)\n",
        "\n",
        "    input = make_batch(input_index)\n",
        "    #print(input.size())\n",
        "\n",
        "    return input, target_index, len(word_to_ix)"
      ],
      "metadata": {
        "id": "PbxQUBqzfAXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input, target_index, vocab_size = Preprocessing(train_data_formatted)\n",
        "val_input, val_target_index, val_vocab_size = Preprocessing(val_data_formatted)\n",
        "test_input, test_target_index, test_vocab_size = Preprocessing(test_data_formatted)"
      ],
      "metadata": {
        "id": "ZgB26YjQfEEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Implementation (Model C)"
      ],
      "metadata": {
        "id": "gCCjF1y40GfN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DuOPNhC3nLXZ"
      },
      "outputs": [],
      "source": [
        "INPUT_SIZE = vocab_size  # source vocab size\n",
        "OUTPUT_SIZE = 3  # target vocab size\n",
        "HIDDEN_SIZE = 512\n",
        "N_LAYERS = 1\n",
        "N_HEADS = 8\n",
        "FF_SIZE = 2048\n",
        "DROPOUT_RATE = 0.6\n",
        "CLIP = 1\n",
        "N_EPOCHS = 50\n",
        "batch_size = 80\n",
        "bptt = batch_size\n",
        "print(batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, scale, dropout_rate):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "        self.scale = scale\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "\n",
        "    def forward(self, query, key, value):\n",
        "        # calculate alignment scores\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1))  # (batch_size, n_heads, query_len, value_len)\n",
        "        scores = scores / self.scale  # (batch_size, num_heads, query_len, value_len)\n",
        "\n",
        "        # calculate the attention weights (prob) from alignment scores\n",
        "        attn_probs = F.softmax(scores, dim=-1)  # (batch_size, n_heads, query_len, value_len)\n",
        "\n",
        "        # calculate context vector\n",
        "        output = torch.matmul(self.dropout(attn_probs), value)  # (batch_size, n_heads, query_len, head_dim)\n",
        "\n",
        "        # output: (batch_size, n_heads, query_len, head_dim)\n",
        "        # attn_probs: (batch_size, n_heads, query_len, value_len)\n",
        "        return output, attn_probs"
      ],
      "metadata": {
        "id": "fPEzKifkVdXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout_rate=0.1):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % n_heads == 0, \"`d_model` should be a multiple of `n_heads`\"\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.d_k = self.d_v = d_model // n_heads  # head_dim\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model, bias=False)  # linear transformation for the Q, K and V vectors\n",
        "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "        self.attention = ScaledDotProductAttention(np.sqrt(self.d_k), dropout_rate)\n",
        "\n",
        "\n",
        "    def project(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        x = x.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)  # (batch_size, n_heads, seq_len, d_k)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "    def unproject(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.n_heads * self.d_k)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "    def forward(self, query, key, value):\n",
        "        # apply linear projections to query, key and value\n",
        "        Q = self.project(self.W_q(query))  # (batch_size, n_heads, query_len, head_dim)\n",
        "        K = self.project(self.W_k(key))  # (batch_size, n_heads, key_len, head_dim)\n",
        "        V = self.project(self.W_v(value))  # (batch_size, n_heads, value_len, head_dim)\n",
        "\n",
        "        # calculate attention weights and context vector for each of the heads\n",
        "        x, attn = self.attention(Q, K, V)\n",
        "\n",
        "        # concatenate context vector of all the heads\n",
        "        x = self.unproject(x)  # (batch_size, query_len, d_model)\n",
        "\n",
        "        # apply linear projection to concatenated context vector\n",
        "        x = self.W_o(x)\n",
        "\n",
        "        return x, attn"
      ],
      "metadata": {
        "id": "Zj2EF_9CVhw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, vocab_size=6000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        encoding = torch.zeros(vocab_size, d_model)\n",
        "        position = torch.arange(0, vocab_size, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, d_model, 2).float()\n",
        "            * (-math.log(10000.0) / d_model)\n",
        "        )\n",
        "        encoding[:, 0::2] = torch.sin(position * div_term)\n",
        "        encoding[:, 1::2] = torch.cos(position * div_term)\n",
        "        encoding = encoding.unsqueeze(0)\n",
        "        self.register_buffer(\"encoding\", encoding)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.encoding[:, : x.size(1), :]\n",
        "        x = self.dropout(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Zg0pHT9PVlTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout_rate=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_ff = d_ff\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        self.w_1 = nn.Linear(d_model, d_ff)\n",
        "        self.w_2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dropout(F.relu(self.w_1(x)))  # (batch_size, seq_len, d_ff)\n",
        "        x = self.w_2(x)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        # x: (batch_size, seq_len, d_model)\n",
        "        return x"
      ],
      "metadata": {
        "id": "1VGlza3yVo5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout_rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_ff = d_ff\n",
        "        self.n_heads = n_heads\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        self.attn_layer = MultiHeadAttention(d_model, n_heads, dropout_rate)\n",
        "        self.attn_layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
        "\n",
        "        self.ff_layer = PositionwiseFeedForward(d_model, d_ff, dropout_rate)\n",
        "        self.ff_layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # apply self-attention\n",
        "        x1, _ = self.attn_layer(x, x, x)  # (batch_size, source_seq_len, d_model)\n",
        "\n",
        "        # apply residual connection followed by layer normalization\n",
        "        x = self.attn_layer_norm(x + self.dropout(x1))  # (batch_size, source_seq_len, d_model)\n",
        "\n",
        "        # apply position-wise feed-forward\n",
        "        x1 = self.ff_layer(x)  # (batch_size, source_seq_len, d_model)\n",
        "\n",
        "        # apply residual connection followed by layer normalization\n",
        "        x = self.ff_layer_norm(x + self.dropout(x1))  # (batch_size, source_seq_len, d_model)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "vJBjV0DxVs7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, n_layers, n_heads, d_ff, pad_idx, dropout_rate=0.1, max_len=5000):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.d_model = d_model\n",
        "        self.n_layers = n_layers\n",
        "        self.d_ff = d_ff\n",
        "        self.pad_idx = pad_idx\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.max_len = max_len\n",
        "\n",
        "        self.tok_embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
        "        self.pos_embedding = PositionalEncoding(d_model, dropout_rate, max_len)\n",
        "        self.layers = nn.ModuleList([\n",
        "            EncoderLayer(d_model, n_heads, d_ff, dropout_rate)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # apply positional encoding to token sequences\n",
        "        x = self.tok_embedding(x)  # (batch_size, source_seq_len, d_model)\n",
        "        x = self.pos_embedding(x)  # (batch_size, source_seq_len, d_model)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)  # (batch_size, source_seq_len, d_model)\n",
        "\n",
        "        x = self.layer_norm(x)  # (batch_size, source_seq_len, d_model)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "l5_GXL_sVwWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout_rate=0.1, num_classes=None):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_ff = d_ff\n",
        "        self.n_heads = n_heads\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.attn_layer = MultiHeadAttention(d_model, n_heads, dropout_rate)\n",
        "        self.attn_layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
        "        self.enc_attn_layer = MultiHeadAttention(d_model, n_heads, dropout_rate)\n",
        "        self.enc_attn_layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
        "        self.ff_layer = PositionwiseFeedForward(d_model, d_ff, dropout_rate)\n",
        "        self.ff_layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Classification layer\n",
        "        self.num_classes = num_classes\n",
        "        if self.num_classes is not None:\n",
        "            self.classifier = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, x, memory):\n",
        "        # apply self-attention\n",
        "        x1, _ = self.attn_layer(x, x, x)\n",
        "\n",
        "        x = self.attn_layer_norm(x + self.dropout(x1))\n",
        "\n",
        "        x1, attn = self.enc_attn_layer(x, memory, memory)\n",
        "\n",
        "        x = self.enc_attn_layer_norm(x + self.dropout(x1))\n",
        "\n",
        "        x1 = self.ff_layer(x)\n",
        "\n",
        "        x = self.ff_layer_norm(x + self.dropout(x1))\n",
        "\n",
        "        return x, attn"
      ],
      "metadata": {
        "id": "x2Djdp53Vznf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, n_layers, n_heads, d_ff, pad_idx, dropout_rate=0.1, max_len=5000):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.d_model = d_model\n",
        "        self.n_layers = n_layers\n",
        "        self.d_ff = d_ff\n",
        "        self.pad_idx = pad_idx\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.max_len = max_len\n",
        "\n",
        "        self.tok_embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
        "        self.pos_embedding = PositionalEncoding(d_model, dropout_rate, max_len)\n",
        "        self.layers = nn.ModuleList([\n",
        "            DecoderLayer(d_model, n_heads, d_ff, dropout_rate)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
        "\n",
        "\n",
        "    def forward(self, x, memory):\n",
        "        #print(\"Input tensor x:\", x)\n",
        "        #print(\"tok_embedding weight size:\", self.tok_embedding.weight.size(0))\n",
        "\n",
        "        # apply positional encoding to token sequences\n",
        "        x = self.tok_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "        x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x, attn = layer(x, memory)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        x = self.layer_norm(x)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        return x, attn"
      ],
      "metadata": {
        "id": "Wp0pSaq-V3Yf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, encoder, decoder, num_classes, pad_idx, d_model):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.pad_idx = pad_idx\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.num_classes = num_classes\n",
        "        self.fc_out = nn.Linear(d_model, self.num_classes)\n",
        "\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        enc_output = self.encoder(src)\n",
        "        dec_output, _ = self.decoder(tgt, enc_output)\n",
        "\n",
        "        # Classification\n",
        "        classification_output = dec_output[:, -1, :]  # Take the final output representation\n",
        "        classification_output = self.fc_out(classification_output)  # Apply a linear layer\n",
        "        #classification_output = nn.functional.softmax(classification_output, dim=-1)  # Apply softmax\n",
        "\n",
        "        # classification_output: (batch_size, num_classes)\n",
        "        return classification_output"
      ],
      "metadata": {
        "id": "fCUgM3d_V7iA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PAD_IDX = 0\n",
        "encoder = Encoder(INPUT_SIZE, HIDDEN_SIZE, N_LAYERS, N_HEADS, FF_SIZE, PAD_IDX, DROPOUT_RATE)\n",
        "decoder = Decoder(INPUT_SIZE, HIDDEN_SIZE, N_LAYERS, N_HEADS, FF_SIZE, PAD_IDX, DROPOUT_RATE)\n",
        "\n",
        "model = Transformer(encoder, decoder, OUTPUT_SIZE, PAD_IDX, HIDDEN_SIZE).to(device)"
      ],
      "metadata": {
        "id": "mj07HPQBWGXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_lr(step_num, warmup_steps=3000, d_model=512):\n",
        "    step_num = max(1, step_num)\n",
        "    arg1 = step_num ** -0.5\n",
        "    arg2 = step_num * warmup_steps ** -1.5\n",
        "    lrate = d_model ** -0.5 * min(arg1, arg2)\n",
        "    return lrate\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=get_lr(0))\n",
        "\n",
        "losses = []\n",
        "accuracies = []\n",
        "\n",
        "\n",
        "def calculate_accuracy(output, targets):\n",
        "    predictions = output.argmax(dim=1, keepdim=True)  # assuming your output includes class scores\n",
        "    correct = predictions.eq(targets.view_as(predictions)).sum().item()\n",
        "    return correct / len(targets)\n",
        "\n",
        "\n",
        "def train(early_stopping=True, patience=25):\n",
        "    model.train() # Turn on the train mode\n",
        "    step = 0\n",
        "    losses = []\n",
        "    accuracies = []\n",
        "    val_losses = []\n",
        "    val_accuracies = []\n",
        "    best_val_loss = float('inf')\n",
        "    best_model_state = None\n",
        "    epochs_without_improvement = 0\n",
        "    for epoch in range(1, N_EPOCHS + 1):\n",
        "      total_loss = 0.0\n",
        "      total_accuracy = 0.0\n",
        "      for batch, i in enumerate(range(0, input.size(0) - 1, bptt)):\n",
        "        data, targets = get_batch(input, torch.tensor(target_index), i, bptt)\n",
        "        #print(data.size(), targets.size())\n",
        "        #print(torch.isnan(data).any(), torch.isinf(data).any())\n",
        "        #print(torch.isnan(targets).any(), torch.isinf(targets).any())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data, data)\n",
        "        for name, param in model.named_parameters():\n",
        "          if not torch.isfinite(param).all():\n",
        "            print(f\"Parameter {name} contains nan or inf.\")\n",
        "        if not torch.isfinite(output).all():\n",
        "          print(\"Output contains nan or inf.\")\n",
        "        if not torch.isfinite(targets).all():\n",
        "          print(\"Targets contain nan or inf.\")\n",
        "        #print(\"OPTSIZE: \", output.size())\n",
        "        #print(output.view(-1, output.size(-1)).size(), targets.view(-1).size())\n",
        "        loss = criterion(output.view(-1, output.size(-1)), targets.view(-1))\n",
        "        #print(loss)\n",
        "        loss.backward()\n",
        "        w = torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
        "        #print(w)\n",
        "\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = get_lr(step + 1)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        #print(\"STEP: \", step)\n",
        "        step += 1\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_accuracy += calculate_accuracy(output, targets)\n",
        "\n",
        "      avg_loss = total_loss / len(range(0, input.size(0) - 1, bptt))\n",
        "      avg_accuracy = total_accuracy / len(range(0, input.size(0) - 1, bptt))\n",
        "      losses.append(avg_loss)\n",
        "      accuracies.append(avg_accuracy)\n",
        "\n",
        "      val_loss, val_accuracy = evaluate(model, val_input, val_target_index, criterion, bptt)\n",
        "      val_losses.append(val_loss)\n",
        "      val_accuracies.append(val_accuracy)\n",
        "\n",
        "      print(f\"Epoch {epoch}: Train Loss = {avg_loss}, Train Accuracy = {avg_accuracy}, Val Loss = {val_loss}, Val Accuracy = {val_accuracy}\")\n",
        "\n",
        "      # Early stopping\n",
        "      if early_stopping:\n",
        "          if val_loss < best_val_loss:\n",
        "              best_val_loss = val_loss\n",
        "              best_model_state = copy.deepcopy(model.state_dict())\n",
        "              epochs_without_improvement = 0\n",
        "          else:\n",
        "              epochs_without_improvement += 1\n",
        "              if epochs_without_improvement >= patience:\n",
        "                  print(f\"Early stopping after {epoch} epochs\")\n",
        "                  break\n",
        "\n",
        "    if early_stopping:\n",
        "        model.load_state_dict(best_model_state)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(losses, label='Training Loss')\n",
        "    plt.title('Training Loss over Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(accuracies, label='Training Accuracy')\n",
        "    plt.title('Training Accuracy over Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    return model, losses, accuracies, val_losses, val_accuracies"
      ],
      "metadata": {
        "id": "YXKhyMALWKHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing and Evaluation (Model C)"
      ],
      "metadata": {
        "id": "STnSgulZ0M3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, test_input, test_target_index, criterion, bptt):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    total_loss = 0.0\n",
        "    total_accuracy = 0.0\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient tracking\n",
        "        for i in range(0, test_input.size(0) - 1, bptt):\n",
        "            data, targets = get_batch(test_input, torch.tensor(test_target_index), i, bptt)\n",
        "            output = model(data, data)\n",
        "            loss = criterion(output.view(-1, output.size(-1)), targets.view(-1))\n",
        "            total_loss += loss.item()\n",
        "            total_accuracy += calculate_accuracy(output, targets)\n",
        "\n",
        "    avg_loss = total_loss / len(range(0, test_input.size(0) - 1, bptt))\n",
        "    avg_accuracy = total_accuracy / len(range(0, test_input.size(0) - 1, bptt))\n",
        "\n",
        "    return avg_loss, avg_accuracy"
      ],
      "metadata": {
        "id": "S2iLNUbM0XT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "def perf_eval(model, test_input, test_target_index, criterion, bptt):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    total_loss = 0.0\n",
        "    total_accuracy = 0.0\n",
        "\n",
        "    all_targets = []\n",
        "    all_predictions = []\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient tracking\n",
        "        for i in range(0, test_input.size(0) - 1, bptt):\n",
        "            data, targets = get_batch(test_input, torch.tensor(test_target_index), i, bptt)\n",
        "            output = model(data, data)\n",
        "            print(output.size())\n",
        "            loss = criterion(output.view(-1, output.size(-1)), targets.view(-1))\n",
        "            total_loss += loss.item()\n",
        "            total_accuracy += calculate_accuracy(output, targets)\n",
        "\n",
        "            # Collect targets and predictions for metrics calculation\n",
        "            all_targets.extend(targets.view(-1).cpu().numpy())\n",
        "            _, predicted = torch.max(output, 1)\n",
        "            all_predictions.extend(predicted.view(-1).cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(range(0, test_input.size(0) - 1, bptt))\n",
        "    avg_accuracy = total_accuracy / len(range(0, test_input.size(0) - 1, bptt))\n",
        "\n",
        "    # Calculate precision, recall, F1 score, and MCC with zero_division set to 0\n",
        "    precision = precision_score(all_targets, all_predictions, average='weighted', zero_division=0)\n",
        "    recall = recall_score(all_targets, all_predictions, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(all_targets, all_predictions, average='weighted', zero_division=0)\n",
        "\n",
        "    return avg_loss, avg_accuracy, precision, recall, f1"
      ],
      "metadata": {
        "id": "9G5NgWoSBk0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, train_losses, train_accuracies, val_losses, val_accuracies = train()\n",
        "\n",
        "test_loss, test_accuracy, precision, recall, f1 = perf_eval(model, test_input, test_target_index, criterion, bptt)\n",
        "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}, Precision: {precision}, Recall: {recall}, F1: {f1}\")"
      ],
      "metadata": {
        "id": "lG9uHomke3oZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}